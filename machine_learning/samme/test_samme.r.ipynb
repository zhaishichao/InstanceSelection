{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T14:57:51.788099Z",
     "start_time": "2025-01-06T14:57:47.042561Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\IDE\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris(return_X_y=True)\n",
    "\n",
    "estimators_ = []\n",
    "learning_rate = 1\n",
    "n_estimators = 5\n",
    "random_state = 100\n",
    "\n",
    "def _boost_real(iboost, X, y, sample_weight, random_state):\n",
    "        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n",
    "        estimator = LogisticRegression(random_state=random_state)\n",
    "        estimators_.append(estimator)      \n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "        y_predict_proba = estimator.predict_proba(X)\n",
    "\n",
    "        classes_ = getattr(estimator, \"classes_\", None)\n",
    "        n_classes_ = len(classes_)\n",
    "\n",
    "        y_predict = classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)\n",
    "        # Instances incorrectly classified\n",
    "        incorrect = y_predict != y\n",
    "        \n",
    "        # Error fraction\n",
    "        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "\n",
    "        # Stop if classification is perfect\n",
    "        if estimator_error <= 0: return sample_weight, 1.0, 0.0\n",
    "\n",
    "        # Construct y coding as described in Zhu et al [2]:\n",
    "        #\n",
    "        #    y_k = 1 if c == k else -1 / (K - 1)\n",
    "        #\n",
    "        # where K == n_classes_ and c, k in [0, K) are indices along the second\n",
    "        # axis of the y coding with c being the index corresponding to the true\n",
    "        # class label.\n",
    "        n_classes = n_classes_\n",
    "        classes = classes_\n",
    "        y_codes = np.array([-1.0 / (n_classes - 1), 1.0])\n",
    "        y_coding = y_codes.take(classes == y[:, np.newaxis])\n",
    "\n",
    "        proba = y_predict_proba  # alias for readability\n",
    "        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n",
    "\n",
    "        # Boost weight using multi-class AdaBoost SAMME.R alg\n",
    "        from scipy.special import xlogy\n",
    "        estimator_weight = (\n",
    "            -1.0\n",
    "            * learning_rate\n",
    "            * ((n_classes - 1.0) / n_classes)\n",
    "            * xlogy(y_coding, y_predict_proba).sum(axis=1)\n",
    "        )\n",
    "        # if iboost==0:\n",
    "        #     print(y_predict_proba,n_classes,classes,y_codes,y_coding,estimator_weight)\n",
    "\n",
    "        # Only boost the weights if it will fit again\n",
    "        if not iboost == n_estimators - 1:\n",
    "            # Only boost positive weights\n",
    "            sample_weight *= np.exp(\n",
    "                estimator_weight * ((sample_weight > 0) | (estimator_weight < 0))\n",
    "            )\n",
    "\n",
    "        return sample_weight, 1.0, estimator_error\n",
    "\n",
    "\n",
    "sample_weight = np.ones(X.shape[0], dtype=np.float64)\n",
    "sample_weight /= sample_weight.sum()\n",
    "# Clear any previous fit results\n",
    "# estimators_ = []\n",
    "estimator_weights_ = np.zeros(n_estimators, dtype=np.float64)\n",
    "estimator_errors_ = np.ones(n_estimators, dtype=np.float64)\n",
    "\n",
    "\n",
    "for iboost in range(n_estimators):\n",
    "    # Boosting step\n",
    "    sample_weight, estimator_weight, estimator_error = _boost_real(\n",
    "        iboost, X, y, sample_weight, random_state\n",
    "    )\n",
    "\n",
    "    # Early termination\n",
    "    if sample_weight is None: break\n",
    "    estimator_weights_[iboost] = estimator_weight\n",
    "    estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "    # Stop if error is zero\n",
    "    if estimator_error == 0: break\n",
    "    sample_weight_sum = np.sum(sample_weight)\n",
    "    if not np.isfinite(sample_weight_sum):\n",
    "        warnings.warn(\n",
    "            \"Sample weights have reached infinite values,\"\n",
    "            f\" at iteration {iboost}, causing overflow. \"\n",
    "            \"Iterations stopped. Try lowering the learning rate.\",\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        break\n",
    "    # Stop if the sum of sample weights has become non-positive\n",
    "    if sample_weight_sum <= 0: break\n",
    "    if iboost < n_estimators - 1: sample_weight /= sample_weight_sum  #normalize\n",
    "    # print(sample_weight)\n",
    "\n",
    "\n",
    "def _samme_proba(estimator, n_classes, X):\n",
    "    proba = estimator.predict_proba(X)\n",
    "    np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n",
    "    log_proba = np.log(proba)\n",
    "    return (n_classes - 1) * (log_proba - (1.0 / n_classes) * log_proba.sum(axis=1)[:, np.newaxis])\n",
    "\n",
    "pred = sum(\n",
    "    _samme_proba(estimator, len(getattr(estimator, \"classes_\", None)), X) for estimator in estimators_\n",
    ")\n",
    "pred /= estimator_weights_.sum()\n",
    "res1 = getattr(estimators_[0], \"classes_\", None).take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(LogisticRegression(), n_estimators= 5, algorithm='SAMME.R')\n",
    "clf.fit(X, y)\n",
    "res2 = clf.predict(X)\n",
    "\n",
    "res1 == res2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
