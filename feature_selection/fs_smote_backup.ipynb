{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 加载数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17b59f066af2813a"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from config import Datasets, Datasets_2\n",
    "from feature_selection import FeatureSelection, non_dominated_sort, train_and_test\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from skfeature.function.information_theoretical_based import CIFE\n",
    "from skfeature.function.statistical_based import chi_square\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # 忽略警告\n",
    "\n",
    "# 数据集\n",
    "fs = FeatureSelection(Datasets_2)\n",
    "# 创建带有列名的空DataFrame\n",
    "df = pd.DataFrame(columns=['数据集', '类分布', '特征数量', '特征选择数量', '原始', 'SMOTE', '特征选择+SMOTE'])\n",
    "for i in range(len(Datasets_2)):\n",
    "    # 数据预处理\n",
    "    fs.pre_process(Datasets_2[i], random_state=42)\n",
    "    print(f\"{i + 1}th dataset: {fs.dataset.DATASETNAME}\")\n",
    "    fs.display_distribution()\n",
    "\n",
    "    # 特征选择（三种不同的选择算法）\n",
    "    idx_1 = fs.feature_selection(fisher_score.fisher_score, mode='index')\n",
    "    # print(idx_1)\n",
    "    idx_2 = fs.feature_selection(chi_square.chi_square, mode='index')\n",
    "    # print(idx_2)\n",
    "    idx_3 = fs.feature_selection(CIFE.cife, mode='index', n_selected_features=fs.x_train.shape[1])\n",
    "    # print(idx_3)\n",
    "\n",
    "    # 非支配排序（三种算法的特征排名）\n",
    "    all_fronts = non_dominated_sort(idx_1, idx_2, idx_3)\n",
    "\n",
    "    # 前后结果对比（原始数据、SMOTE、特征选择+SMOTE）\n",
    "    model = MLPClassifier(hidden_layer_sizes=(fs.dataset.HIDDEN_SIZE,), max_iter=fs.dataset.MAX_ITER,\n",
    "                          random_state=42, learning_rate_init=fs.dataset.LEARNING_RATE)\n",
    "    res_1 = train_and_test(clone(model), fs.x_train, fs.x_test, fs.y_train, fs.y_test)\n",
    "    print(f\"原始：{res_1}\")\n",
    "    x_train, y_train = SMOTE(random_state=42, k_neighbors=fs.dataset.K_NEIGHBORS).fit_resample(fs.x_train, fs.y_train)\n",
    "    res_2 = train_and_test(clone(model), x_train, fs.x_test, y_train, fs.y_test)\n",
    "    print(f\"SMOTE：{res_2}\")\n",
    "    x_train, y_train = SMOTE(random_state=42, k_neighbors=fs.dataset.K_NEIGHBORS).fit_resample(\n",
    "        fs.x_train[:, all_fronts[0]], fs.y_train)\n",
    "    res_3 = train_and_test(clone(model), x_train, fs.x_test[:, all_fronts[0]], y_train, fs.y_test)\n",
    "    print(f\"特征选择+SMOTE：{res_3}\")\n",
    "    # 保存结果\n",
    "    df.loc[i] = [fs.dataset.DATASETNAME, fs.distribution, fs.x_train.shape[1],\n",
    "                 len(all_fronts[0]), res_1, res_2, res_3]\n",
    "#  保存结果\n",
    "df.to_csv('feature_selection_result.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T09:13:57.002770Z",
     "start_time": "2025-06-12T08:39:22.871544Z"
    }
   },
   "id": "3a1b77a2d1e48a90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th dataset: Armstrong-2002-v1.mat\n",
      "trainset distribution: [17 33]\n",
      "testset distribution: [ 7 15]\n",
      "number of feature: 1081\n",
      "原始：(0.966092, 1.0, 0.949425)\n",
      "SMOTE：(1.0, 1.0, 1.0)\n",
      "特征选择+SMOTE：(0.730297, 0.87619, 0.770833)\n",
      "2th dataset: Gordon-2002.mat\n",
      "trainset distribution: [ 22 104]\n",
      "testset distribution: [ 9 46]\n",
      "number of feature: 1626\n",
      "原始：(0.989071, 1.0, 0.96819)\n",
      "SMOTE：(1.0, 1.0, 1.0)\n",
      "特征选择+SMOTE：(1.0, 1.0, 1.0)\n",
      "3th dataset: Colon.mat\n",
      "trainset distribution: [15 28]\n",
      "testset distribution: [ 7 12]\n",
      "number of feature: 2000\n",
      "原始：(0.886405, 0.916667, 0.886905)\n",
      "SMOTE：(0.886405, 0.916667, 0.886905)\n",
      "特征选择+SMOTE：(0.763763, 0.880952, 0.736842)\n",
      "4th dataset: Yeoh-2002-v1.mat\n",
      "trainset distribution: [143  30]\n",
      "testset distribution: [62 13]\n",
      "number of feature: 2526\n",
      "原始：(0.889698, 0.952854, 0.868267)\n",
      "SMOTE：(0.825313, 0.954094, 0.871619)\n",
      "特征选择+SMOTE：(0.897335, 0.98139, 0.887082)\n",
      "5th dataset: DLBCL.mat\n",
      "trainset distribution: [40 13]\n",
      "testset distribution: [18  6]\n",
      "number of feature: 5469\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m idx_1 \u001B[38;5;241m=\u001B[39m fs\u001B[38;5;241m.\u001B[39mfeature_selection(fisher_score\u001B[38;5;241m.\u001B[39mfisher_score, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# print(idx_1)\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m idx_2 \u001B[38;5;241m=\u001B[39m \u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_selection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchi_square\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchi_square\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mindex\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# print(idx_2)\u001B[39;00m\n\u001B[0;32m     29\u001B[0m idx_3 \u001B[38;5;241m=\u001B[39m fs\u001B[38;5;241m.\u001B[39mfeature_selection(CIFE\u001B[38;5;241m.\u001B[39mcife, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m'\u001B[39m, n_selected_features\u001B[38;5;241m=\u001B[39mfs\u001B[38;5;241m.\u001B[39mx_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32mD:\\Develop\\WorkSpace\\Python\\InstanceSelection\\feature_selection\\fs.py:50\u001B[0m, in \u001B[0;36mFeatureSelection.feature_selection\u001B[1;34m(self, func, **kwargs)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeature_selection\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 50\u001B[0m     index \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# 按照特征的优先级排序（index[0]表示最重要的特征）\u001B[39;00m\n\u001B[0;32m     51\u001B[0m     index_convert \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_ranking(index)  \u001B[38;5;66;03m# 没有顺序，index_convert[0]表示第一个特征的重要性排名（越小越重要）\u001B[39;00m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m index_convert\n",
      "File \u001B[1;32mD:\\IDE\\Anaconda\\envs\\pytorch\\lib\\site-packages\\skfeature\\function\\statistical_based\\chi_square.py:32\u001B[0m, in \u001B[0;36mchi_square\u001B[1;34m(X, y, mode)\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmode is not one of \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrank\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ()\n\u001B[1;32m---> 32\u001B[0m F, pval \u001B[38;5;241m=\u001B[39m \u001B[43mchi2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\n",
      "File \u001B[1;32mD:\\IDE\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    214\u001B[0m         )\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    226\u001B[0m     )\n",
      "File \u001B[1;32mD:\\IDE\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:268\u001B[0m, in \u001B[0;36mchi2\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m    266\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(X, accept_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m(np\u001B[38;5;241m.\u001B[39mfloat64, np\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39many((X\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;28;01mif\u001B[39;00m issparse(X) \u001B[38;5;28;01melse\u001B[39;00m X) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m--> 268\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput X must be non-negative.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    270\u001B[0m \u001B[38;5;66;03m# Use a sparse representation for Y by default to reduce memory usage when\u001B[39;00m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;66;03m# y has many unique classes.\u001B[39;00m\n\u001B[0;32m    272\u001B[0m Y \u001B[38;5;241m=\u001B[39m LabelBinarizer(sparse_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mfit_transform(y)\n",
      "\u001B[1;31mValueError\u001B[0m: Input X must be non-negative."
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
